# User Guide Evaluation Tool

Automated evaluation system for user guides based on accessibility, actionability, and learning effectiveness.

## Quick Start

```bash
# Evaluate a user guide
npx tsx scripts/eval-user-guide.ts docs/binto-labs/guides/claude-flow-user-guide-2025-10-14.md

# Using npm script
npm run eval-guide docs/binto-labs/guides/your-guide.md
```

## What It Evaluates

The tool scores guides on **5 categories** (100 points total):

### 1. Structure & Navigation (20 points)
- Table of contents with time estimates
- Progressive complexity (Quick Start ‚Üí Advanced)
- Visual scanning aids (emoji, tags)
- Cross-references and links

### 2. Example Quality (30 points)
- Complete examples (Scenario, Goal, Command, Output)
- Real-world scenarios (not toy examples)
- Production-ready code with error handling
- Before/after comparisons

### 3. Actionability (25 points)
- Quick wins (5-minute success)
- Customization guidance
- Verification steps
- Troubleshooting section

### 4. Learning Path (15 points)
- Natural concept progression
- Cross-references to related content
- Multiple learning styles (visual, hands-on, conceptual)

### 5. Accessibility (10 points)
- Conversational tone
- Short paragraphs (‚â§4 sentences)
- Beginner-friendly
- Encourages experimentation

## Scoring Interpretation

- **90-100**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **Exceptional** - Reference-quality resource
- **80-89**: ‚≠ê‚≠ê‚≠ê‚≠ê **Excellent** - Strong guide, minor improvements needed
- **70-79**: ‚≠ê‚≠ê‚≠ê **Good** - Functional but could be more accessible
- **60-69**: ‚≠ê‚≠ê **Adequate** - Covers basics, needs polish
- **Below 60**: ‚≠ê **Needs Improvement** - Major revision recommended

## Output

The tool generates:

1. **Console Report**: Immediate feedback with scores and suggestions
2. **Markdown Report**: Saved as `[guide-name]-EVAL-REPORT.md`

### Example Report

```markdown
# User Guide Evaluation Report

**Guide:** claude-flow-user-guide-2025-10-14.md
**Date:** 2025-10-16

## Overall Score
**92/100** (92.0%)
**Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Exceptional

## Content Metrics
- Word Count: 9,212
- Examples: 20
- Code Blocks: 124
- Cross-References: 53
- Time to First Win: 1 minute

## Category Scores
### Structure & Navigation: 20/20 (100%)
‚úÖ Perfect structure with TOC, time estimates, and visual aids

### Example Quality: 27/30 (90%)
‚úÖ Complete examples with real-world scenarios
‚ö†Ô∏è Suggestion: Remove placeholder text from code

### Actionability: 25/25 (100%)
‚úÖ Excellent quick start and customization guidance

### Learning Path: 13/15 (87%)
‚úÖ Strong progression and cross-references
‚ö†Ô∏è Suggestion: Add architecture diagrams

### Accessibility: 7/10 (70%)
‚úÖ Short paragraphs, beginner-friendly
‚ö†Ô∏è Suggestion: Use more conversational language

## Key Strengths
1. Comprehensive coverage (20 examples)
2. Rich code examples (124 blocks)
3. Excellent cross-referencing (53 links)
4. Quick wins (1-minute first success)

## Priority Improvements
1. Add architecture diagrams for complex examples
2. Use more conversational language (Let's, Here's how)
```

## Metrics Tracked

### Content Metrics
- **Word Count**: Total words in guide
- **Example Count**: Number of working examples
- **Code Block Count**: Copy-paste code snippets
- **Cross-Reference Count**: Internal links
- **Time to First Win**: Minutes from start to first success

### Quality Indicators
- **Readability**: Average paragraph length (target: ‚â§4 sentences)
- **Completeness**: Examples with all required sections
- **Real-World**: Production scenarios vs. toy examples
- **Actionability**: Quick Start section + verification steps

## Integration with CI/CD

Add to GitHub Actions workflow:

```yaml
- name: Evaluate User Guide
  run: |
    npx tsx scripts/eval-user-guide.ts docs/user-guide.md
    if [ $? -ne 0 ]; then
      echo "User guide quality below threshold (70%)"
      exit 1
    fi
```

## Evaluation Criteria

Full criteria documented in:
- [USER-GUIDE-EVAL-CRITERIA.md](../docs/binto-labs/guides/USER-GUIDE-EVAL-CRITERIA.md)

## Use Cases

### For Documentation Authors
‚úÖ Check quality before publishing (aim for 85+)
‚úÖ Identify gaps in coverage
‚úÖ Track improvement over time

### For Reviewers
‚úÖ Objective scoring for pull requests
‚úÖ Specific, actionable feedback
‚úÖ Compare guides consistently

### For Users
‚úÖ Assess guide quality before investing time
‚úÖ Find best resources quickly
‚úÖ Contribute improvements

## Example: Improving a Guide from 72 ‚Üí 92

**Initial Score: 72/100** ‚≠ê‚≠ê‚≠ê Good
- Missing Quick Start section
- No time estimates
- Incomplete examples (no verification steps)
- No cross-references

**Actions Taken:**
1. ‚úÖ Added Quick Start (5 min to success)
2. ‚úÖ Added time estimates to all examples
3. ‚úÖ Added "Verify it worked" sections
4. ‚úÖ Added 30+ cross-references
5. ‚úÖ Split into progressive parts

**Final Score: 92/100** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Exceptional

## Tips for High Scores

### Structure (20 pts)
- ‚úÖ Start with comprehensive TOC
- ‚úÖ Add time estimates: `[10-min]`
- ‚úÖ Add difficulty tags: `[basic]`, `[advanced]`
- ‚úÖ Use emoji for visual scanning üöÄ‚ö°üîí

### Examples (30 pts)
- ‚úÖ Use template: **Scenario** ‚Üí **Goal** ‚Üí **Command** ‚Üí **Output**
- ‚úÖ Show expected output
- ‚úÖ Explain "How it works"
- ‚úÖ Add "Verify it worked" steps
- ‚úÖ Use real-world scenarios
- ‚úÖ Include before/after code

### Actionability (25 pts)
- ‚úÖ Quick Start section (‚â§5 min examples)
- ‚úÖ "Customize it" variations
- ‚úÖ Troubleshooting section
- ‚úÖ Cross-references to related topics

### Learning Path (15 pts)
- ‚úÖ Progress from simple ‚Üí complex
- ‚úÖ Cross-reference related examples
- ‚úÖ Link to deep-dives for advanced users

### Accessibility (10 pts)
- ‚úÖ Conversational tone (Let's, Here's how)
- ‚úÖ Short paragraphs (‚â§4 sentences)
- ‚úÖ Define jargon on first use
- ‚úÖ Encourage experimentation

## Comparison with Traditional Docs

| Traditional Documentation | This Evaluation System |
|--------------------------|------------------------|
| ‚ùå Academic tone | ‚úÖ Conversational, accessible |
| ‚ùå Theory-heavy | ‚úÖ Copy-paste examples first |
| ‚ùå Linear only | ‚úÖ Multiple learning paths |
| ‚ùå No time estimates | ‚úÖ Clear time expectations |
| ‚ùå No validation | ‚úÖ "Verify it worked" steps |
| ‚ùå Toy examples | ‚úÖ Real-world scenarios |

## Development

### Run Tests
```bash
# Evaluate multiple guides
npm run eval-guide docs/binto-labs/guides/*.md

# Compare scores
find docs -name "*-EVAL-REPORT.md" -exec grep "Overall Score" {} \;
```

### Customize Scoring
Edit `scripts/eval-user-guide.ts`:
- Adjust category weights
- Add new evaluation criteria
- Modify rating thresholds

### Extend Metrics
Add new metrics to `calculateMetrics()`:
- Readability scores (Flesch Reading Ease)
- Complexity analysis
- Sentiment analysis
- Video/screenshot count

## Future Enhancements

- [ ] Automated readability scoring (Flesch-Kincaid)
- [ ] Screenshot detection and counting
- [ ] Link validation (broken links)
- [ ] Code example syntax validation
- [ ] Comparative analysis (guide vs. guide)
- [ ] Historical tracking (scores over time)
- [ ] AI-powered suggestions (GPT-4)

## Reference

**Based on analysis of:**
- `claude-flow-user-guide-2025-10-14.md` (Score: 92/100)
- Best practices from technical writing research
- User feedback and success metrics

**Evaluation criteria:**
- [USER-GUIDE-EVAL-CRITERIA.md](../docs/binto-labs/guides/USER-GUIDE-EVAL-CRITERIA.md)

---

**Version:** 1.0.0
**Last Updated:** 2025-10-16
